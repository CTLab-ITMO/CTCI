{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:53:13.280473Z",
     "start_time": "2024-10-18T16:53:13.271073Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from segment_anything.utils.transforms import ResizeLongestSide"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = 'mps'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-18T16:54:03.004287Z",
     "start_time": "2024-10-18T16:54:02.997700Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:45:11.778446Z",
     "start_time": "2024-10-18T16:44:46.447425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load SAM model\n",
    "model_type = \"vit_h\"  # Model type can be \"vit_b\", \"vit_l\", or \"vit_h\"\n",
    "sam = sam_model_registry[model_type](checkpoint=\"../../models/annotation/sam_vit_h_4b8939.pth\")\n",
    "sam.to('mps')  # Use 'cpu' if you don't have a GPU\n",
    "\n",
    "# Create a predictor instance\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_grid_points(image_size, points_per_side):\n",
    "    height, width = image_size\n",
    "    x_points = torch.linspace(0, width, points_per_side)\n",
    "    y_points = torch.linspace(0, height, points_per_side)\n",
    "    \n",
    "    # Create grid of points\n",
    "    grid_points = torch.cartesian_prod(x_points, y_points)\n",
    "    \n",
    "    return grid_points"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-18T16:57:54.080376Z",
     "start_time": "2024-10-18T16:57:54.052368Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m predictor\u001B[38;5;241m.\u001B[39mset_image(image)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Run the predictor with the grid points\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m masks, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mpredictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoint_coords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_points\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpoint_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmultimask_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/segment_anything/predictor.py:154\u001B[0m, in \u001B[0;36mSamPredictor.predict\u001B[0;34m(self, point_coords, point_labels, box, mask_input, multimask_output, return_logits)\u001B[0m\n\u001B[1;32m    151\u001B[0m     mask_input_torch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(mask_input, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    152\u001B[0m     mask_input_torch \u001B[38;5;241m=\u001B[39m mask_input_torch[\u001B[38;5;28;01mNone\u001B[39;00m, :, :, :]\n\u001B[0;32m--> 154\u001B[0m masks, iou_predictions, low_res_masks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_torch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoords_torch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels_torch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbox_torch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask_input_torch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmultimask_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_logits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m masks \u001B[38;5;241m=\u001B[39m masks[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    164\u001B[0m iou_predictions \u001B[38;5;241m=\u001B[39m iou_predictions[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/segment_anything/predictor.py:238\u001B[0m, in \u001B[0;36mSamPredictor.predict_torch\u001B[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001B[0m\n\u001B[1;32m    229\u001B[0m low_res_masks, iou_predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmask_decoder(\n\u001B[1;32m    230\u001B[0m     image_embeddings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures,\n\u001B[1;32m    231\u001B[0m     image_pe\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mprompt_encoder\u001B[38;5;241m.\u001B[39mget_dense_pe(),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    234\u001B[0m     multimask_output\u001B[38;5;241m=\u001B[39mmultimask_output,\n\u001B[1;32m    235\u001B[0m )\n\u001B[1;32m    237\u001B[0m \u001B[38;5;66;03m# Upscale the masks to the original image resolution\u001B[39;00m\n\u001B[0;32m--> 238\u001B[0m masks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpostprocess_masks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlow_res_masks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moriginal_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_logits:\n\u001B[1;32m    241\u001B[0m     masks \u001B[38;5;241m=\u001B[39m masks \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmask_threshold\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/segment_anything/modeling/sam.py:154\u001B[0m, in \u001B[0;36mSam.postprocess_masks\u001B[0;34m(self, masks, input_size, original_size)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpostprocess_masks\u001B[39m(\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    135\u001B[0m     masks: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m    136\u001B[0m     input_size: Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m],\n\u001B[1;32m    137\u001B[0m     original_size: Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m],\n\u001B[1;32m    138\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    139\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m    Remove padding and upscale masks to the original image size.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;124;03m        is given by original_size.\u001B[39;00m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m     masks \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmasks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimg_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimg_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbilinear\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43malign_corners\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m     masks \u001B[38;5;241m=\u001B[39m masks[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, : input_size[\u001B[38;5;241m0\u001B[39m], : input_size[\u001B[38;5;241m1\u001B[39m]]\n\u001B[1;32m    161\u001B[0m     masks \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39minterpolate(masks, original_size, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, align_corners\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/functional.py:4038\u001B[0m, in \u001B[0;36minterpolate\u001B[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001B[0m\n\u001B[1;32m   4032\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mare_deterministic_algorithms_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mis_cuda:\n\u001B[1;32m   4033\u001B[0m             \u001B[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001B[39;00m\n\u001B[1;32m   4034\u001B[0m             \u001B[38;5;66;03m# importlib is required because the import cannot be top level\u001B[39;00m\n\u001B[1;32m   4035\u001B[0m             \u001B[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001B[39;00m\n\u001B[1;32m   4036\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtorch._decomp.decompositions\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mupsample_bilinear2d_vec(\n\u001B[1;32m   4037\u001B[0m                 \u001B[38;5;28minput\u001B[39m, output_size, align_corners, scale_factors)\n\u001B[0;32m-> 4038\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupsample_bilinear2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malign_corners\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_factors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4039\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   4040\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m align_corners \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Load the image (you can replace this with your own image path)\n",
    "image_path = r'../../data/split/train/images/F1_1_1_2.ts-frames_frame-1635.png'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "points_per_side = 32  # You can modify this number to control the density of points\n",
    "\n",
    "\n",
    "image_size = image.shape[:2]\n",
    "grid_points = generate_grid_points(image_size, points_per_side)\n",
    "\n",
    "# Prepare the input points for the predictor\n",
    "input_points = grid_points.numpy()\n",
    "input_labels = np.ones(input_points.shape[0])  # 1 for foreground points\n",
    "\n",
    "predictor.set_image(image)\n",
    "\n",
    "# Run the predictor with the grid points\n",
    "masks, _, _ = predictor.predict(point_coords=input_points, point_labels=input_labels, multimask_output=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-18T17:00:46.776809Z",
     "start_time": "2024-10-18T16:57:54.526432Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def visualize_grid_points(grid_points):\n",
    "    # Convert grid points to x, y for plotting\n",
    "    x_coords, y_coords = grid_points[:, 0].numpy(), grid_points[:, 1].numpy()\n",
    "    \n",
    "    # Plot the points on the image\n",
    "    plt.scatter(x_coords, y_coords, s=10, c='red', marker='o')  # s is size, c is color, marker is shape\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call the function to visualize\n",
    "visualize_grid_points(grid_points.cpu())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[0].cpu(), cmap='gray')\n",
    "plt.title('Generated Mask')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {masks.shape[0]} masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
