{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as tf\n",
    "\n",
    "from src.models.swin.model import Swin\n",
    "from src.models.train import Trainer\n",
    "from src.features.segmentation.dataset import SegmentationDataset\n",
    "\n",
    "from transformers import AutoImageProcessor, Swinv2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\")\n",
    "image_processor.do_resize = False\n",
    "image_processor.do_rescale = False\n",
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin = Swin(\n",
    "    net=model,\n",
    "    image_processor=image_processor,\n",
    "    device=\"mps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "swin = swin.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "for name, param in swin.named_parameters():\n",
    "    if param.device != \"mps:0\":\n",
    "        print(param.device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tf.Resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDataset(\n",
    "    images_dir=\"../data/split/train/train_images/\",\n",
    "    masks_dir=\"../data/split/train/train_masks/\",\n",
    "    image_transform=transform,\n",
    "    mask_transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    images_dir=\"../data/split/valid/valid_images/\",\n",
    "    masks_dir=\"../data/split/valid/valid_masks/\",\n",
    "    image_transform=transform,\n",
    "    mask_transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5855, 0.5000],\n",
      "          ...,\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.6402, 0.8030, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5509, 0.6979, 0.5000]]],\n",
      "\n",
      "\n",
      "        [[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5363, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.6248, 0.5000],\n",
      "          ...,\n",
      "          [0.5000, 0.5083, 0.5299,  ..., 0.6543, 0.8282, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5220, 0.6688, 0.5000]]]],\n",
      "       device='mps:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for image, target in train_dataloader:\n",
    "    image = image.to(device)\n",
    "    target = target.to(device)\n",
    "    out = swin.val_on_batch(image, target)\n",
    "    print(out[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
