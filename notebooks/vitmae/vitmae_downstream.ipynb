{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "139e95c475643a5b"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:32.087738200Z",
     "start_time": "2024-01-31T09:41:32.053828900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTMAEModel, AutoImageProcessor\n",
    "\n",
    "from src.features.vitmae.dataset import init_downstream_datasets, init_dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e33ec114b482582"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:32.318630500Z",
     "start_time": "2024-01-31T09:41:32.060809700Z"
    }
   },
   "id": "825cf21d8b4e4c32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image Processor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee7587f4535b0aad"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "image_processor_checkpoint = r\"facebook/vit-mae-base\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(image_processor_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:32.462345800Z",
     "start_time": "2024-01-31T09:41:32.137605900Z"
    }
   },
   "id": "31edcfc17cd621f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4bb97b3dcedf19"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "train_images_dir = r\"\"\n",
    "val_images_dir = r\"\"\n",
    "train_masks_dir = r\"\"\n",
    "val_masks_dir = r\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:32.478302900Z",
     "start_time": "2024-01-31T09:41:32.463342900Z"
    }
   },
   "id": "348843f7cb1e3fe"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "batch_size_train = 8\n",
    "batch_size_val = 8\n",
    "pin_memory = True\n",
    "num_workers = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:32.509220300Z",
     "start_time": "2024-01-31T09:41:32.481294600Z"
    }
   },
   "id": "db23bd2c1320ee60"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = init_downstream_datasets(\n",
    "    train_images_dir=train_images_dir,\n",
    "    val_images_dir=val_images_dir,\n",
    "    train_masks_dir=train_masks_dir,\n",
    "    val_masks_dir=val_masks_dir,\n",
    "    image_processor=image_processor\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:32.509220300Z",
     "start_time": "2024-01-31T09:41:32.496256700Z"
    }
   },
   "id": "3c0b2593d6e7d5b0"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = init_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size_train=batch_size_train,\n",
    "    batch_size_val=batch_size_val,\n",
    "    pin_memory=pin_memory,\n",
    "    num_workers=num_workers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:32.528170600Z",
     "start_time": "2024-01-31T09:41:32.510219600Z"
    }
   },
   "id": "261d1694a4f44f2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b4ebd2977b1b3d1"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Internship\\ITMO_ML\\CTCI\\checkpoints\\vit\\vitmae_on_bubbles\\run3\\epoch_10_config\\\\ were not used when initializing ViTMAEModel: ['decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.mask_token', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_before.bias']\n",
      "- This IS expected if you are initializing ViTMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ViTMAEModel.from_pretrained(r\"\")\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.201790700Z",
     "start_time": "2024-01-31T09:41:32.523183800Z"
    }
   },
   "id": "e8be31b76f383a"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = targets.float()\n",
    "        # outputs = outputs.view(-1)\n",
    "        # targets = targets.view(-1)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        ce_loss = criterion(inputs, targets)\n",
    "        ce_exp = torch.exp(-ce_loss)\n",
    "        focal_loss = (self.alpha * (1 - ce_exp) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.215754100Z",
     "start_time": "2024-01-31T09:41:34.204785Z"
    }
   },
   "id": "fb70367ff3099020"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class Conv3x3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 dropout=False, batch_norm=False, instance_norm=False, activation_func=None, bias=True):\n",
    "        super(Conv3x3, self).__init__()\n",
    "        self.net = self._init_net(in_channels, out_channels,\n",
    "                                  dropout, batch_norm, instance_norm, bias, activation_func)\n",
    "\n",
    "    def _init_net(self, in_channels, out_channels, dropout, batch_norm, instance_norm, bias, activation_func):\n",
    "        \n",
    "        net_list = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=bias)]\n",
    "        if dropout:\n",
    "            net_list.append(nn.Dropout2d(p=0.33))\n",
    "        if batch_norm:\n",
    "            net_list.append(nn.BatchNorm2d(out_channels))\n",
    "        if instance_norm:\n",
    "            net_list.append(nn.InstanceNorm2d(out_channels))\n",
    "        if activation_func:\n",
    "            net_list.append(activation_func)\n",
    "        net = nn.Sequential(*net_list)\n",
    "        return net\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.245675100Z",
     "start_time": "2024-01-31T09:41:34.220743500Z"
    }
   },
   "id": "d3cb54602b163168"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class SegmentModel(torch.nn.Module):\n",
    "    def __init__(self, vitmae):\n",
    "        super().__init__()\n",
    "        self.encoder = vitmae\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=150, out_channels=150, kernel_size=2, stride=2),\n",
    "            Conv3x3(in_channels=150, out_channels=100, batch_norm=False, activation_func=nn.LeakyReLU()), # 32 x 32\n",
    "            nn.ConvTranspose2d(in_channels=100, out_channels=100, kernel_size=2, stride=2),\n",
    "            Conv3x3(in_channels=100, out_channels=64, batch_norm=False, activation_func=nn.LeakyReLU()), # 64 x 64\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2),\n",
    "            Conv3x3(in_channels=64, out_channels=32, batch_norm=False, activation_func=nn.LeakyReLU()),  # 128 x 128\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=2, stride=2),\n",
    "            Conv3x3(in_channels=32, out_channels=16, batch_norm=False, activation_func=nn.LeakyReLU())  # 256 x 256\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, _, num_channels, height, width = inputs.data[\"pixel_values\"].shape\n",
    "        inputs.data[\"pixel_values\"] = torch.reshape(\n",
    "            inputs.data[\"pixel_values\"],\n",
    "            (batch_size, num_channels, height, width)\n",
    "        )\n",
    "        outputs = self.encoder(**inputs)\n",
    "        last_hidden_state  = outputs.last_hidden_state\n",
    "        features = torch.reshape(last_hidden_state, (batch_size, 150, 16, 16))\n",
    "        features = self.decoder(features)\n",
    "        \n",
    "        mask = self.classifier(features)\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.256645600Z",
     "start_time": "2024-01-31T09:41:34.234704900Z"
    }
   },
   "id": "9564bc122a85638d"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "segmentator = SegmentModel(vitmae=model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.265622400Z",
     "start_time": "2024-01-31T09:41:34.248667900Z"
    }
   },
   "id": "88fb671920015889"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.296538600Z",
     "start_time": "2024-01-31T09:41:34.264623800Z"
    }
   },
   "id": "4d402f470c4a2f99"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "segmentation_criterion = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.345552500Z",
     "start_time": "2024-01-31T09:41:34.281577800Z"
    }
   },
   "id": "26e35acd4514ad44"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "save_dir = r\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.356522700Z",
     "start_time": "2024-01-31T09:41:34.343557100Z"
    }
   },
   "id": "99c93d2ef2733f0a"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.383451200Z",
     "start_time": "2024-01-31T09:41:34.357520100Z"
    }
   },
   "id": "c090033c77f507d4"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def downstream_task_train(\n",
    "        model, optimizer, segmentation_criterion,\n",
    "        train_dataloder, val_dataloader,\n",
    "        device, save_dir,\n",
    "        num_epochs=5\n",
    "):\n",
    "    history = {\"train_batch\": [], \"train_epoch\": [], \"val_batch\": [], \"val_epoch\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model = model.to(device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        epoch_history = {\"train\": [], \"val\": []}\n",
    "\n",
    "        model.train()\n",
    "        for inputs, masks in tqdm(train_dataloder):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = segmentator(inputs)\n",
    "            loss = segmentation_criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            history[\"train_batch\"].append(loss.item())\n",
    "            epoch_history[\"train\"].append(loss.item())\n",
    "\n",
    "        epoch_train_loss = sum(epoch_history[\"train\"]) / len(epoch_history[\"train\"])\n",
    "        print(f\"Epoch train loss: {epoch_train_loss}\")\n",
    "        history[\"train_epoch\"].append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        for inputs, masks in tqdm(val_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = segmentator(inputs)\n",
    "            loss = segmentation_criterion(outputs, masks)\n",
    "\n",
    "            history[\"val_batch\"].append(loss.item())\n",
    "            epoch_history[\"val\"].append(loss.item())\n",
    "\n",
    "        epoch_val_loss = sum(epoch_history[\"val\"]) / len(epoch_history[\"val\"])\n",
    "        print(f\"Epoch val loss: {epoch_val_loss}\\n\")\n",
    "        history[\"val_epoch\"].append(epoch_val_loss)\n",
    "\n",
    "        save_model(model.to(\"cpu\"), path=os.path.join(save_dir, f\"epoch_{epoch + 1}.pt\"))\n",
    "\n",
    "    return history\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T09:41:34.394423100Z",
     "start_time": "2024-01-31T09:41:34.373477800Z"
    }
   },
   "id": "aa760bef9fef663c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = downstream_task_train(\n",
    "    model=segmentator,\n",
    "    optimizer=optimizer,\n",
    "    segmentation_criterion=segmentation_criterion,\n",
    "    train_dataloder=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    num_epochs=5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "782c03bf00c755fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(18, 6)\n",
    "\n",
    "ax[0].plot(range(len(history[\"train_epoch\"])), history[\"train_epoch\"])\n",
    "ax[0].set_title(\"Train loss\")\n",
    "ax[1].plot(range(len(history[\"val_epoch\"])), history[\"val_epoch\"])\n",
    "ax[1].set_title(\"Val loss\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa9056b9cb5f4c52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for inputs, masks in train_dataloader:\n",
    "    inputs = inputs\n",
    "    outputs = segmentator(inputs)\n",
    "    mask = outputs[0].squeeze().detach().numpy()\n",
    "    plt.imshow(mask)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac191bb6991e2b3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c423761565637cb8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
