{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T13:23:42.010868Z",
     "start_time": "2025-02-12T13:23:38.537862Z"
    }
   },
   "outputs": [],
   "source": [
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "import cv2\n",
    "from src.utils.masks import masks_narrowing, unite_masks\n",
    "from src.annotation.yolo import load_yolo_sahi_detector\n",
    "from src.annotation.sam import sam_segmentation, load_sam_predictor\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def yolo_sahi_detect(\n",
    "        image,\n",
    "        detector,\n",
    "        shape_scale=2,\n",
    "        slice_scale=4,\n",
    "        overlap_ratio=0.1,\n",
    "        postprocess_type='NMS'\n",
    "):\n",
    "    original_h, original_w = image.shape[:2]\n",
    "    resized_h, resized_w = original_h * shape_scale, original_w * shape_scale\n",
    "\n",
    "    image_resized = cv2.resize(image, (resized_w, resized_h))\n",
    "\n",
    "    result = get_sliced_prediction(\n",
    "        image_resized,\n",
    "        detector,\n",
    "        slice_height=resized_h // slice_scale,\n",
    "        slice_width=resized_w // slice_scale,\n",
    "        overlap_height_ratio=overlap_ratio,\n",
    "        overlap_width_ratio=overlap_ratio,\n",
    "        postprocess_type=postprocess_type,\n",
    "    )\n",
    "\n",
    "    object_prediction_list = result.object_prediction_list\n",
    "    boxes = []\n",
    "\n",
    "    scale_x = original_w / resized_w\n",
    "    scale_y = original_h / resized_h\n",
    "\n",
    "    for object_prediction in object_prediction_list:\n",
    "        x1, y1, x2, y2 = object_prediction.bbox.to_xyxy()\n",
    "\n",
    "        x1 = int(x1 * scale_x)\n",
    "        y1 = int(y1 * scale_y)\n",
    "        x2 = int(x2 * scale_x)\n",
    "        y2 = int(y2 * scale_y)\n",
    "\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    return boxes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-12T13:23:42.016963Z",
     "start_time": "2025-02-12T13:23:42.012312Z"
    }
   },
   "id": "2899b55d96492ce6",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor's device is cpu\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"../data/covdor/192.168.1.11_2024-12-07T17:58:07.png\")\n",
    "detector = load_yolo_sahi_detector(\"../models/annotation/11rocks.pt\")\n",
    "predictor = load_sam_predictor(\"../models/annotation/sam_vit_b_01ec64.pth\", model_type=\"vit_b\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-12T13:23:43.720890Z",
     "start_time": "2025-02-12T13:23:42.017981Z"
    }
   },
   "id": "3710ab2b929ac141",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Гиперпараметры для грид-серча\n",
    "shape_scales = [1, 2, 3]\n",
    "slice_scales = [2, 4, 6]\n",
    "overlap_ratios = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "postprocess_types = ['NMM', 'GREEDYNMM', 'NMS']\n",
    "\n",
    "param_combinations = list(itertools.product(shape_scales, slice_scales, overlap_ratios, postprocess_types))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-12T13:23:43.732270Z",
     "start_time": "2025-02-12T13:23:43.726661Z"
    }
   },
   "id": "5947172b6c466b86",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 4 slices.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 12\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, param_combination \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(param_combinations):\n\u001B[1;32m      3\u001B[0m     boxes \u001B[38;5;241m=\u001B[39m yolo_sahi_detect(\n\u001B[1;32m      4\u001B[0m         image\u001B[38;5;241m=\u001B[39mimage,\n\u001B[1;32m      5\u001B[0m         detector\u001B[38;5;241m=\u001B[39mdetector,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m         postprocess_type\u001B[38;5;241m=\u001B[39mparam_combination[\u001B[38;5;241m3\u001B[39m],\n\u001B[1;32m     10\u001B[0m     )\n\u001B[0;32m---> 12\u001B[0m     masks_list \u001B[38;5;241m=\u001B[39m \u001B[43msam_segmentation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredictor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpredictor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mboxes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mboxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_points\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     masks_united \u001B[38;5;241m=\u001B[39m [unite_masks(masks) \u001B[38;5;28;01mfor\u001B[39;00m masks \u001B[38;5;129;01min\u001B[39;00m masks_list]\n\u001B[1;32m     16\u001B[0m     masks_narrowed \u001B[38;5;241m=\u001B[39m masks_narrowing(masks_united, narrowing\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/src/annotation/sam.py:118\u001B[0m, in \u001B[0;36msam_segmentation\u001B[0;34m(image, predictor, boxes, prompt_points, target_length)\u001B[0m\n\u001B[1;32m    110\u001B[0m     masks_list, _, _ \u001B[38;5;241m=\u001B[39m predictor\u001B[38;5;241m.\u001B[39mpredict_torch(\n\u001B[1;32m    111\u001B[0m         boxes\u001B[38;5;241m=\u001B[39mboxes_tensor,\n\u001B[1;32m    112\u001B[0m         point_coords\u001B[38;5;241m=\u001B[39mpoints_tensor,\n\u001B[1;32m    113\u001B[0m         point_labels\u001B[38;5;241m=\u001B[39mlabels_tensor,\n\u001B[1;32m    114\u001B[0m         multimask_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    115\u001B[0m     )\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 118\u001B[0m     masks_list, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mpredictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_torch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mboxes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mboxes_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmultimask_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpoint_coords\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpoint_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m masks_list \u001B[38;5;241m=\u001B[39m masks_list\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m masks_list\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/segment_anything/predictor.py:229\u001B[0m, in \u001B[0;36mSamPredictor.predict_torch\u001B[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001B[0m\n\u001B[1;32m    222\u001B[0m sparse_embeddings, dense_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mprompt_encoder(\n\u001B[1;32m    223\u001B[0m     points\u001B[38;5;241m=\u001B[39mpoints,\n\u001B[1;32m    224\u001B[0m     boxes\u001B[38;5;241m=\u001B[39mboxes,\n\u001B[1;32m    225\u001B[0m     masks\u001B[38;5;241m=\u001B[39mmask_input,\n\u001B[1;32m    226\u001B[0m )\n\u001B[1;32m    228\u001B[0m \u001B[38;5;66;03m# Predict masks\u001B[39;00m\n\u001B[0;32m--> 229\u001B[0m low_res_masks, iou_predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmask_decoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprompt_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dense_pe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m    \u001B[49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    233\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdense_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    234\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmultimask_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmultimask_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;66;03m# Upscale the masks to the original image resolution\u001B[39;00m\n\u001B[1;32m    238\u001B[0m masks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpostprocess_masks(low_res_masks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moriginal_size)\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/segment_anything/modeling/mask_decoder.py:94\u001B[0m, in \u001B[0;36mMaskDecoder.forward\u001B[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     73\u001B[0m     image_embeddings: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     77\u001B[0m     multimask_output: \u001B[38;5;28mbool\u001B[39m,\n\u001B[1;32m     78\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m     79\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;124;03m    Predict masks given image and prompt embeddings.\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;124;03m      torch.Tensor: batched predictions of mask quality\u001B[39;00m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 94\u001B[0m     masks, iou_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_masks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_pe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# Select the correct mask or masks for outptu\u001B[39;00m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m multimask_output:\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/segment_anything/modeling/mask_decoder.py:132\u001B[0m, in \u001B[0;36mMaskDecoder.predict_masks\u001B[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001B[0m\n\u001B[1;32m    129\u001B[0m b, c, h, w \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    131\u001B[0m \u001B[38;5;66;03m# Run the transformer\u001B[39;00m\n\u001B[0;32m--> 132\u001B[0m hs, src \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_src\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m iou_token_out \u001B[38;5;241m=\u001B[39m hs[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[1;32m    134\u001B[0m mask_tokens_out \u001B[38;5;241m=\u001B[39m hs[:, \u001B[38;5;241m1\u001B[39m : (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_mask_tokens), :]\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/segment_anything/modeling/transformer.py:92\u001B[0m, in \u001B[0;36mTwoWayTransformer.forward\u001B[0;34m(self, image_embedding, image_pe, point_embedding)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;66;03m# Apply transformer blocks and final layernorm\u001B[39;00m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 92\u001B[0m     queries, keys \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m        \u001B[49m\u001B[43mqueries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpoint_embedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_pe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# Apply the final attenion layer from the points to the image\u001B[39;00m\n\u001B[1;32m    100\u001B[0m q \u001B[38;5;241m=\u001B[39m queries \u001B[38;5;241m+\u001B[39m point_embedding\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/segment_anything/modeling/transformer.py:180\u001B[0m, in \u001B[0;36mTwoWayAttentionBlock.forward\u001B[0;34m(self, queries, keys, query_pe, key_pe)\u001B[0m\n\u001B[1;32m    178\u001B[0m attn_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcross_attn_image_to_token(q\u001B[38;5;241m=\u001B[39mk, k\u001B[38;5;241m=\u001B[39mq, v\u001B[38;5;241m=\u001B[39mqueries)\n\u001B[1;32m    179\u001B[0m keys \u001B[38;5;241m=\u001B[39m keys \u001B[38;5;241m+\u001B[39m attn_out\n\u001B[0;32m--> 180\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m queries, keys\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217\u001B[0m, in \u001B[0;36mLayerNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 217\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/CTCI/venv/lib/python3.10/site-packages/torch/nn/functional.py:2900\u001B[0m, in \u001B[0;36mlayer_norm\u001B[0;34m(input, normalized_shape, weight, bias, eps)\u001B[0m\n\u001B[1;32m   2890\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[1;32m   2891\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   2892\u001B[0m         layer_norm,\n\u001B[1;32m   2893\u001B[0m         (\u001B[38;5;28minput\u001B[39m, weight, bias),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2898\u001B[0m         eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m   2899\u001B[0m     )\n\u001B[0;32m-> 2900\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2901\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[1;32m   2902\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for idx, param_combination in enumerate(param_combinations):\n",
    "\n",
    "    boxes = yolo_sahi_detect(\n",
    "        image=image,\n",
    "        detector=detector,\n",
    "        shape_scale=param_combination[0],\n",
    "        slice_scale=param_combination[1],\n",
    "        overlap_ratio=param_combination[2],\n",
    "        postprocess_type=param_combination[3],\n",
    "    )\n",
    "\n",
    "    masks_list = sam_segmentation(\n",
    "        image=image, predictor=predictor, boxes=boxes, prompt_points=False, target_length=1024\n",
    "    )\n",
    "    masks_united = [unite_masks(masks) for masks in masks_list]\n",
    "    masks_narrowed = masks_narrowing(masks_united, narrowing=0.2)\n",
    "    mask_sam = unite_masks(masks_narrowed)\n",
    "\n",
    "    mask = mask_sam[:, :, np.newaxis].repeat(3, axis=2)\n",
    "    \n",
    "    alpha = 0.6\n",
    "    vis = image * alpha + (1 - alpha) * mask\n",
    "    vis = vis.astype(np.uint8)\n",
    "    \n",
    "    cv2.imwrite(f\"../data/exp/sh_{param_combination[0]},sl_{param_combination[1]},op_{param_combination[2]},p_ {param_combination[3]}.png\", vis)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-12T13:24:00.631953Z",
     "start_time": "2025-02-12T13:23:43.734475Z"
    }
   },
   "id": "c17e18e33c167bdc",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-12T13:24:00.633270Z",
     "start_time": "2025-02-12T13:24:00.633086Z"
    }
   },
   "id": "f34a7b8d5c7c6176"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
