"""
data_annotation.py

This script provides image annotation using YOLOv8 and Segment Anything Model (SAM).
It provides a command-line interface for annotating a folder of images with bounding boxes
generated by YOLOv8 and segmentation masks produced by the SAM model.

Usage:
    python data_annotation.py [DATA_DIR] [FOLDER] [YOLO_CHECKPOINT] [SAM_CHECKPOINT] [SAM_MODEL_TYPE]

Arguments:
    - DATA_DIR (str): Directory containing the input images for annotation.
    - FOLDER (str): Name of the folder containing the images.
    - YOLO_CHECKPOINT (str): Path to the custom YOLOv8 checkpoint file.
    - SAM_CHECKPOINT (str): Path to the Segment Anything Model (SAM) checkpoint file.
    - SAM_MODEL_TYPE (str): Type of the SAM model, choose from ["vit_b", "vit_l", "vit_h"].

Optional Arguments:
    - --target_length (int): Target length for resizing the longest side of the image (default: 1024).
    - --narrowing (float): Narrowing factor for SAM masks (default: 0.20).
    - --erode_iterations (int): Number of iterations for eroding the final mask (default: 1).
    - --processes_num (int): Number of parallel processes for segmentation (default: 0).
    - --device (str): Device for processing ("cpu" or "cuda") (default: "cpu").

Notes:
    - Progress information is printed during annotation.
    - Segmented masks are saved to the output directory.
    - If processes_num is set to 0, sequential annotation is performed.
    - If processes_num is greater than 0, parallel annotation is performed using concurrent.futures (only on cpu).
"""

import os
import hydra
import torch
from omegaconf import DictConfig

from src.annotation.watershed import Watershed
from src.annotation.yolo import load_yolov8_detector
from src.annotation.sam import load_sam_predictor, segment_images_from_folder


@hydra.main(version_base=None, config_path='../../configs', config_name='annotation')
def run_annotation(cfg: DictConfig) -> None:
    source_dir = os.path.join(cfg.data_dir, cfg.folder)
    output_dir = os.path.join(cfg.data_dir, cfg.folder + "_masks")

    detector = None
    predictor = None
    wshed = None

    # Validate and load YOLO + SAM components if present in config
    if "sam_yolo" in cfg:
        sy_cfg = cfg.sam_yolo

        # Validate SAM model type
        if sy_cfg.sam.model_type not in ["vit_b", "vit_l", "vit_h"]:
            raise ValueError(f"Undefined SAM model type: {sy_cfg.sam.model_type}")

        # Load YOLOv8 detector
        detector = load_yolov8_detector(sy_cfg.yolo_checkpoint_path)

        # Optimize PyTorch for GPU if applicable
        if cfg.device != "cpu":
            torch.cuda.empty_cache()
            torch.backends.cuda.matmul.allow_tf32 = False
            torch.backends.cudnn.benchmark = True

        # Load SAM predictor
        predictor = load_sam_predictor(device=cfg.device, **sy_cfg.sam)

    # Load Watershed if preprocess is defined in config
    if "preprocess" in cfg:
        watershed_cfg = cfg.preprocess
        wshed = Watershed(watershed_cfg)

    # Ensure at least one segmentation method is defined
    if detector is None and wshed is None:
        raise ValueError("At least one segmentation method (Watershed or YOLO + SAM) must be defined in the config.")

    # Segment images
    segment_images_from_folder(
        source_dir=source_dir,
        output_dir=output_dir,
        detector=detector,
        predictor=predictor,
        wshed=wshed,
        target_length=sy_cfg.target_length if "sam_yolo" in cfg else None,
        narrowing=sy_cfg.narrowing if "sam_yolo" in cfg else None,
        erode_iterations=sy_cfg.erode_iterations if "sam_yolo" in cfg else None,
        prompt_points=sy_cfg.prompt_points if "sam_yolo" in cfg else None,
    )


if __name__ == "__main__":
    run_annotation()